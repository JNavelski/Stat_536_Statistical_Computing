---
title: "Navelski Homework 1"
author: "J. Navelski"
date: "9/16/2020"
output: html_document
---

\usepackage[labelfont={bf}]{caption} 
\usepackage[unicode=true, breaklinks=true]{hyperref}
\userpackage{lmodern}
\usepackage{amssymb}
\usepackage{changepage}
\usepackage{eurosym}
\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=cyan,
}

\newcommand{\Lagr}{\mathcal{L}}
\newcommand{\Lagr}{L}
\usepackage{ amssymb }
\mathcal{L}




```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

```

```{r cars}
summary(cars)
```



## Question 1
#### Deriving the $E(X)$ for a Binomial RV

Let, \[X \sim Bin(x;n,p)\]
Then \[E(X) = E\left(\binom{n}{x} \cdot p^x(1-p)^{n-x} \right)\]
\[ = \sum_{x=0}^{n}x\binom{n}{x} \cdot p^x(1-p)^{n-x}  \]
Where, we know that the term where $x=0$ vanishes, and the final term of the $x!$ in the denominator cancels s.t.
\[ = \sum_{x=1}^{n}\frac{n!}{(x-1)!(n-x)!} \cdot p^x (1-p)^{n-x}  \]
This implies that we can let $y=x-1$ and $m=n-1$, and thus substitute for $x-1$ and $n=m+1$ into the last sum (and using the fact that the limits of $x=1$ and $x=n$ correspond to $y=0$ and $y=n-1=m$, respectivly) we get
\[ = \sum_{y=0}^{m}\frac{(m+1)!}{y!(m-y)!} \cdot p^{y+1}(1-p)^{m-y}  \]
\[ = \sum_{y=0}^{m}\frac{(m+1)!}{y!(m-y)!} \cdot p^{y+1}(1-p)^{m-y}  \]
Where we can pull out the $(m+1)$ and $p$ s.t.
\[ = (m+1)p\sum_{y=0}^{m}\frac{(m)!}{y!(m-y)!} \cdot p^{y}(1-p)^{m-y}  \]
and the Binomial Therom says
\[ (a+b)^{m}= \sum_{y=0}^{m}\frac{(m)!}{y!(m-y)!} \cdot a^{y}b^{m-y}  \]
and thus implies that
\[\implies  \hspace{.5cm} E(X) = (m+1)p(p+1-p)^{m}=np  \]
since $n-1=m$ and $(p+1-p)^{m}=1$.
\\
\\
\\
\vspace{2cm}
 
#### Deriving the $V(X)$ for a Binomial RV

Simalarly to above, we can use a trick where we find $E(X(X-1))$ using the assumption that $y=x-2$ and $m=n-2$ s.t.
\[E(X(X-1))=  \sum_{x=0}^{n}x(x-1)\binom{n}{x} \cdot p^x(1-p)^{n-x} \]
\[ = \sum_{x=2}^{n}\frac{n!}{(x-2)!(n-x)!} \cdot p^x(1-p)^{n-x}  \]
\[ = n(n-1)p^{2} \sum_{x=2}^{n}\frac{(n-2)!}{(x-2)!(n-x)!} \cdot (1-p)^{x-2}q^{n-x}  \]
\[ = n(n-1)p^{2} \sum_{y=0}^{m}\frac{(m)!}{y!(m-y)!} \cdot p^{y}(1-p)^{m-y} \]
\[ = n(n-1)p^{2} (p+1-p)^{m} \]
\[\implies \hspace{.5cm} E(X(X-1)) = n(n-1)p^{2} \]
That said, using the variance formula we can get the useful form of
\[ V(X) = E[(X-E(X))(X-E(X))]\]
\[ = E[X^{2}-2XE(X)+E(X)^{2}]\]
\[ = E(X^{2})-E(X)^{2}\]
\[ = E(X^{2})+E(X)-E(X)-E(X)^{2}\]
\[ = E(X(X-1))+E(X)-E(X)^{2}\]
and inserting our known results
\[ = n(n-1)p^{2}+np-n^{2}p^{2}\]
\[ =  n^{2}p^{2}-np^{2}+np-n^{2}p^{2}\]
\[\implies \hspace{.5cm}V(X) =  np(1-p) \]

## Question 2
#### Deriving the MLE of an Exponential Distribution and a Normal Distribution
$2.1)$ MLE of Exponetial Dist.

Let a sample $x_{i}, \dots, x_{n}$ be drawn from a random variable $X\sim Exp(\lambda)$ with a probability drawn from a PDF of 

  \begin{equation}
    f_{X}(x_{1},\dots,x_{i},\dots,x_{n})=
    \begin{cases}
      \lambda exp(-\lambda  x_{i}), & \text{if}\ x_{i} \in  [0,\infty)\\
      0, & \text{otherwise}
    \end{cases}
  \end{equation}

To get the MLE of if this distribution we first use the joint probability distribution to get the likelihood function s.t.
\[\Lagr (\lambda ; x_{1},\dots,x_{n}) =  \prod_{i=1}^{n}f(x_{i};\lambda)    \]
\[ =  \prod_{i=1}^{n}\left(\lambda exp(-\lambda  x_{i})\right)  \]
\[ =  \lambda^{n}exp\left(- \lambda \sum_{i=1}^{n}x_{i}  \right) \]
and we log the liklihood of the function to make calculations easier s.t.
\[ \mathcal{L} (\lambda ; x_{1},\dots,x_{n}) =    log\left(\lambda^{n}exp\left(- \lambda \sum_{i=1}^{n}x_{i}  \right) \right)   \]
\[  =   log\left(\lambda^{n}\right) - \lambda \sum_{i=1}^{n}x_{i}\]
Taking the derivite of the log-likelihood function with respect to the distribution's parameter $(\lambda)$ will ensure that we choose the optimal parameter that maximizes the joint probability of observing those data points s.t.
\[  \frac{\partial \mathcal{L} (\lambda ; x_{1},\dots,x_{n})}{\partial \lambda} = \frac{n}{\lambda}-\sum_{i=1}^{n}x_{i}  \hspace{.5cm} =  \hspace{.5cm} 0 \]
\[ \frac{n}{\lambda} = \sum_{i=1}^{n}x_{i} \]
\[ \implies \lambda_{MLE} \equiv \hat{\lambda} = \frac{n}{\sum_{i=1}^{n}x_{i}} \]
which is just the recipocal of the sample mean such that
\[ \hat{\lambda} =  \frac{1}{\overline{x}}   \]


$2.2)$ MLE of Normal Dist. with know variance ($\sigma$)


Let a sample $x_{i}, \dots, x_{n}$ be drawn from a random variable $X\sim \mathcal{N} (\mu,\sigma^{2})$, where $\sigma_{2}$ is know, with a probability drawn from a PDF of 

  \begin{equation}
    f_{X}(x_{1},\dots,x_{i},\dots,x_{n})=
    \begin{cases}
      \frac{1}{\sigma \sqrt{2\pi}} exp\left(-\frac{1}{2}\left( \frac{x_{i}-\mu}{\sigma} \right)^{2}\right), & \text{if}\ x_{i} \in  (-\infty,\infty) \text{ and } \ \sigma^{2} \in  [0,\infty)  \\
      0, & \text{otherwise}
    \end{cases}
  \end{equation}

To get the MLE of if this distribution we first use the joint probability distribution to get the likelihood function s.t.
\[\Lagr (\mu , \sigma^{2} ; x_{1},\dots,x_{n}) =  \prod_{i=1}^{n}f(x_{i};\mu,\sigma^{2})    \]
\[ =  \prod_{i=1}^{n}\left( \frac{1}{\sigma\sqrt{2\pi}} exp\left(-\frac{1}{2}\left( \frac{x_{i}-\mu}{\sigma} \right)^{2}\right)\right)  \]
\[ =  \left( \frac{n}{\sigma\sqrt{2\pi}} exp\left(-\frac{1}{2\sigma^{2}}\sum_{i=1}^{n}\left(x_{i}-\mu \right)^{2}\right)\right)  \]
\[ =  \frac{n}{\sigma\sqrt{2\pi}} exp\left(-\frac{1}{2\sigma^{2}}\sum_{i=1}^{n}\left(x_{i}-\mu \right)^{2}\right) \]
and we log the liklihood of the function to make calculations easier s.t.
\[ \mathcal{L} (\mu, \sigma^{2} ; x_{1},\dots,x_{n}) =    log\left(  \frac{n}{\sigma\sqrt{2\pi}} exp\left(-\frac{1}{2\sigma^{2}}\sum_{i=1}^{n}\left(x_{i}-\mu \right)^{2}\right)  \right)   \]
\[  =   -\frac{n}{2}log\left(\sqrt{2\pi}\right) -\frac{n}{2}log(\sigma^{2}) -\frac{1}{2\sigma^{2}} \sum_{i=1}^{n} \left( x_{i}-\mu \right)^{2}    \]
Taking the derivite of the log-likelihood function with respect to the distribution's parameter $(\mu)$ will ensure that we choose the optimal parameter that maximizes the joint probability of observing those data points s.t.
\[  \frac{\partial \mathcal{L} (\lambda ; x_{1},\dots,x_{n})}{\partial \mu} =   -2\frac{1}{2\sigma^{2}} \sum_{i=1}^{n} \left( x_{i}-\mu \right) (-1)   =  \hspace{.5cm} 0 \]
\[    \frac{1}{2\sigma^{2}} \sum_{i=1}^{n} \left( x_{i}-\mu \right)  =  \hspace{.5cm} 0 \]
\[    \sum_{i=1}^{n} \left( x_{i}\right)  =  \sum_{i=1}^{n} \left( \mu \right)\]
\[ \implies \mu_{MLE} \equiv  \hat{\mu} =  \frac{\sum_{i=1}^{n} x_{i}}{n}    \]

## Question 3

## Question 4


```{r}

x = matrix(0,nrow = 1000, ncol = 4)
p=.25
for(i in 1:1000){
  for(j in 1:3){
    x[i,j] = runif(n=1,0,1)
    x[i,j] = ifelse(x[i,j]<(1-p),0,1)
  }
}

for(i in 1:1000){
      x[i,4]=sum(x[i,1:3])
}

hist(x[,4], main = "Histogram of 1000 Bin(n=3,p=0.25) Obvservations")


```


## Question 5

```{r}

alpha = 4
beta = 1

x = matrix(0,nrow = 1000, ncol = alpha+1)

for(i in 1:1000){
  for(j in 1:4){
      x[i,j] = runif(1,0,1)
  }
}

for(i in 1:1000){
      x[i,5]=-beta*(sum(log(x[i,1:alpha])))
}

hist(x[,5], main = "Histogram of 1000 Gam(alpha=4,beta=1) Obvservations")

```




## Question 6
## Question 7

#### Part A

```{r}




```


#### Part B
Consider that the instrumental variable $Y$ to be distributed $Y \sim \mathcal{U}[0,1]$ and generate a sample size of $n$ from $X$, where $X$ is distributed $X \sim Beta(c+1,c+1)$, using the Accept-Reject (AR) method.

Let,
\[  h(X) \equiv \frac{f(X)}{g(X)} = \frac{1}{B(c+1,c+1)}X^{(c+1)-1}(1-X)^{(c+1)-1}  \]
\[ = \frac{(X(X-1))^{c}}{B(c+1,c+1)}   \]
since $g(X) = 1$ and simplifying.  We can take the log and find the optimal $M$ implicitly through finding the supremum of $h(X)$ s.t.
\[ log(h(X)) =  log \left(\frac{(X(X-1))^{c}}{B(c+1,c+1)}\right)  \]
\[ log(h(X)) = c*log(X)+c*log(1-X)-log\left(    B(c+1,c+1)  \right) \]
Taking the first order conditions
\[ \frac{\partial log(h(X))}{\partial X} = \frac{c}{X}-\frac{c}{1-X} \hspace{.5cm}=\hspace{.5cm}0\]
\[  \implies X^{*} = \frac{1}{2}   \]
we get the optimal value of $X$ that allows us to choose the optimal value of $M$ s.t.
\[M^{*}(X^{*})=h(X^{*}=\frac{1}{2})=\frac{\left(\frac{1}{2}\left(\frac{1}{2}-1\right)\right)^{c}}{B(c+1,c+1)}  \]
\[ = \frac{\left(\frac{1}{4}\right)^{c}}{B(c+1,c+1)}  \]
\[\implies \hspace{.5cm} M^{*}(X^{*})  = \left(\frac{1}{4}\right)^{c} \frac{1}{B(c+1,c+1)}  \]
Using this condition on $M^* ( X^* )$ we can the formulate the AR algorythim as:
\vspace{2cm}
Accept realization if
\[ u_{1} \leq \frac{\left(  \frac{(X(X-1))^{c}}{B(c+1,c+1)}  \right)   }{\left( \left(\frac{1}{4}\right)^{c} \frac{1}{B(c+1,c+1)}  \right)  } \hspace{.5cm} \equiv \hspace{.5cm} \frac{f(X)}{M^{*}g(X)}    \]
\[ u_{2} \leq 4^{c}(X(1-X))^{c} \hspace{.5cm} \text{where }X\sim\mathcal{U}(0,1) \text{ and } c \text{ is a constant}  \]
and reject and redraw otherwise.

## Question 8

```{r}


## Normal distribution using the Box-Mueller method
## Note that we are going from two 

normal_sim=function(nsim){
  ns=ceiling(nsim/2)
  u1=runif(ns,0,1)
  u2=runif(ns,0,1)
  x1=(sqrt(-2*log(u1)))*cos(2*pi*u2)
  x2=(sqrt(-2*log(u1)))*sin(2*pi*u2)
  x=c(x1,x2)[1:nsim]
  return(x)
}

### Any Univariate Normal Distribution
# once we have a N(0,1) we can simulate any univeriate
# or multivariate normal distribution we can define any univariate
# normal distribution as x~N(0,1) ---> z=mu+si*x ~ N(mu,si^2)
norm_sim=function(nsim,mu=0,var=1){
  x=normal_sim(nsim)
  z=mu+(sqrt(var))*x
  return(z)
}

z=norm_sim(1000,mu=3,var=2)

h<-hist(z, xlab="Realizations of Z", main="Histogram with Normal(3,4) w/ a fitted Normal Curve Kernal", ylim=c(0,300)) 
zfit<-seq(min(z),max(z),length=40) 
yfit<-dnorm(zfit,mean=mean(z),sd=sd(z))
yfit <- yfit*diff(h$mids[1:2])*length(z) 
lines(zfit, yfit, col="blue", lwd=2)



## Normal distribution using the Marsaglia's polar method

nsim=2000
ns=ceiling((nsim/2))
u1=runif(ns,-1,1)
u2=runif(ns,-1,1)
s=u1^2 + u2^2
y=cbind(u1,u2,s)


for (i in 1:ns){
  while(y[i,3]>=1){
    y[i,1] = runif(1,-1,1)
    y[i,2] = runif(1,-1,1)
    y[i,3] = y[i,1]^2 + y[i,2]^2
    if(y[i,3]< 1 & 0<y[i,3])
    break()
}}

x1=matrix(0,nrow=ns,ncol=1)
x2=matrix(0,nrow=ns,ncol=1)

for(i in 1:ns){
  x1[i,1]=sqrt((-2*log(y[i,3])))*(y[i,1]/sqrt(y[i,3]))
  x2[i,1]=sqrt((-2*log(y[i,3])))*(y[i,2]/sqrt(y[i,3]))
  }

hist(x1)
hist(x2)

```

## Question 9

Consider that the instrumental variable $Y$, distributed $Y \sim Exp(\lambda)$ and generate a sample size of $n$ from $X$, where $X$ is distributed $X \sim Gam(a,b)$, using the Accept-Reject (AR) method.

Let,
\[  h(X) \equiv \frac{f(X)}{g(X)} = \frac{b^a}{\Gamma(a)\lambda  }X^{a-1}e^{X(\lambda-b)}  \]
since $g(X) = \lambda e^{-\lambda X}$, $f(X) = \frac{b^a}{\Gamma(a) }X^{a-1}e^{-bX}$ and simplifying.  We can take the log and find the optimal $M$ implicitly through finding the supremum of $h(X)$ s.t.
\[ log(h(X)) =  log \left(\frac{b^a}{\Gamma(a)\lambda  }X^{a-1}e^{X(\lambda-b)} \right)  \]
\[ log(h(X)) = a*log(b)-log( \Gamma(a) \lambda)+ (a -1 )log(X) + X(\lambda-b) \]
Taking the first order conditions
\[ \frac{\partial log(h(X))}{\partial X} = \frac{(a-1)}{X}+(\lambda-b) \hspace{.5cm}=\hspace{.5cm}0\]
\[  \implies X^{*} = \frac{a-1}{b-\lambda}   \]
we get the optimal value of $X$, which is now a function of the endogenous parameter $\lambda$.  Using this $X^{*}$, we can characterize the optimal value of $M$ implicitly through choosing the optimal value of $\lambda$ s.t.
\[M(X^{*}(\lambda))=h(X^{*}(\lambda))=   \frac{b^a}{\Gamma(a)\lambda  } \left(\frac{a-1}{b-\lambda}\right)^{a-1}e^{\frac{a-1}{b-\lambda}(\lambda-b)}  \]
where we can take the log of to simplify the derivitive
\[ log(M(\lambda)) = log \left(  \frac{b^a}{\Gamma(a)}\right)-log(\lambda)+ (a -1 )log(a-1) - (a-1)log(b-\lambda)-(b-\lambda)\frac{a-1}{b-\lambda} \]
\[ \frac{\partial log(M(\lambda))}{\partial \lambda} = -\frac{1}{\lambda}-\frac{(a-1)}{(b-\lambda)}(-1) \hspace{.5cm}=\hspace{.5cm}0\]
\[ \frac{(a-1)}{(b-\lambda)}=\frac{1}{\lambda} \]
\[  \implies \lambda^{*} = \frac{b}{a}  \text{ .}\]
Using this $\lambda^{*}$, we can characterize the optimal value of $M$ s.t.
\[M^{*}(X^{*}(\lambda^*))  = \frac{b^a}{\Gamma(a)(\frac{b}{a})} \left(\frac{a-1}{b-(\frac{b}{a})}\right)^{a-1}e^{-\frac{a-1}{b-\frac{b}{a}}(b-\frac{b}{a})}  \]
\[= \frac{1}{\Gamma(a)} a^{a} e^{-(a-1)}  \]
Using this condition on $M^*(X^*(\lambda^{*}) )$ we can the formulate the AR algorythim as:
\vspace{2cm}

Accept realization if
\[ u_{1} \leq \left(  \frac{b^a}{\Gamma(a)\lambda  }X^{a-1}e^{X(\lambda-b)}  \right)\left(  \frac{1}{\frac{1}{\Gamma(a)} a^{a} e^{-(a-1)}}  \right)  \hspace{.5cm} \equiv \hspace{.5cm} \frac{h(X)}{M^{*}} \hspace{.5cm} \equiv \hspace{.5cm} \frac{f(X)}{M^{*}g(X)}    \]
\[ u_{1} \leq \left( \frac{b}{a} \right)^{a-1} X^{(a-1)} e^{X(\frac{b}{a} - b)+(a-1)} \hspace{.5cm} \text{where }X\sim Exp(\lambda=\frac{b}{a})  \]
and reject and redraw otherwise.






## Question 10










\newpage